{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 9. Optimización\n",
    "Guillermo Segura Gómez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1 \n",
    "\n",
    "Construir un clasificador binario basado en el método de regresión logística.\n",
    "Puede revisar las notas de las ayudantías 10 y 11. En particular,\n",
    "podemos tomar de referencia el artículo (minka-logreg.pdf) que aparece en la Ayudantía 10:\n",
    "\n",
    "> \"A comparison of numerical optimizers for logistic regression\". Thomas P. Minka\n",
    "\n",
    "Para usar la notación de este artículo, tenemos un conjunto de datos y cada dato\n",
    "puede pertener a una de dos clases. \n",
    "Las clases  se identifican con las etiquetas \"-1\" y \"1\". Para hacer la clasificación\n",
    "se necesita determinar un vector $\\mathbf{w}$ que se usa para\n",
    "calcular la probabilidad de que un dato $\\mathbf{x}_i \\in \\mathbb{R}^n$ \n",
    "pertenezca a la clase $y_i \\in \\{-1,1\\}$ mediante la evaluación de la función sigmoide:\n",
    "\n",
    "$$ \\sigma(\\mathbf{x}_i, y_i, \\mathbf{w}) = \\frac{1}{1 + \\exp(-y_i\\mathbf{w}^\\top\\mathbf{x}_i)}.  $$ \n",
    "\n",
    "Cada vector $\\mathbf{x}_i$ está formado por el valor de ciertas características\n",
    "asociadas el individuo $i$-ésimo.\n",
    "\n",
    "Dada una colección de datos etiquetados $(\\mathbf{x}_1, y_1), ..., (\\mathbf{x}_m, y_m)$, se \n",
    "mide el error de clasificación mediante\n",
    "\n",
    "$$ L(\\mathbf{w}) = \\sum_{i=1}^m \\log(1 + \\exp(-y_i\\mathbf{w}^\\top\\mathbf{x}_i)) \n",
    "+ \\frac{\\lambda}{2} \\mathbf{w}^\\top\\mathbf{w}.  $$ \n",
    "\n",
    "El segundo término de la expresión anterior penaliza la magnitud de la solución $\\mathbf{w}$\n",
    "dependiendo del valor de $\\lambda$.\n",
    "\n",
    "En general, los datos se almacenan en una matriz de modo de cada vector $\\mathbf{x}_i$\n",
    "es una fila de la matriz $\\mathbf{X}$ y las etiquetas $y_i$ son las componentes de un\n",
    "vector $\\mathbf{y}$:\n",
    "\n",
    "$$ \\mathbf{X} = \\left[ \\begin{array}{c} \n",
    "\\mathbf{x}_1^\\top \\\\ \n",
    "\\mathbf{x}_2^\\top \\\\ \n",
    "\\vdots \\\\\n",
    "\\mathbf{x}_m^\\top \n",
    "\\end{array} \\right], \\qquad\n",
    "\\mathbf{y} = \\left( \\begin{array}{c} \n",
    "y_1 \\\\ \n",
    "y_2 \\\\ \n",
    "\\vdots \\\\\n",
    "y_m \n",
    "\\end{array} \\right).\n",
    "$$\n",
    "\n",
    "1. Muestre que el gradiente de $L(\\mathbf{w})$ está dado por\n",
    "\n",
    "$$ \\nabla_w L(\\mathbf{w}) = - \\sum_{i=1}^m (1 - \\sigma(\\mathbf{x}_i, y_i, \\mathbf{w})) y_i\\mathbf{x}_i\n",
    "+ \\lambda \\mathbf{w}. $$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar el gradiente de la función de pérdida $L(\\mathbf{w})$ comenzaremos calculando la derivada de $L(\\mathbf{w})$ con respecto a cada componente de $\\mathbf{w}$. La función $L(\\mathbf{w})$ está definida como:\n",
    "\n",
    "$$ L(\\mathbf{w}) = \\sum_{i=1}^m \\log(1 + \\exp(-y_i\\mathbf{w}^\\top\\mathbf{x}_i)) + \\frac{\\lambda}{2} \\mathbf{w}^\\top\\mathbf{w} $$\n",
    "\n",
    "La derivada de $\\log(1 + \\exp(-y_i\\mathbf{w}^\\top\\mathbf{x}_i))$ con respecto a $\\mathbf{w}$ se puede encontrar usando la regla de la cadena:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\mathbf{w}} \\log(1 + \\exp(-y_i\\mathbf{w}^\\top\\mathbf{x}_i)) = \\frac{1}{1 + \\exp(-y_i\\mathbf{w}^\\top\\mathbf{x}_i)} \\cdot \\exp(-y_i\\mathbf{w}^\\top\\mathbf{x}_i) \\cdot \\frac{\\partial}{\\partial \\mathbf{w}} (-y_i\\mathbf{w}^\\top\\mathbf{x}_i) $$\n",
    "\n",
    "Aquí, $\\frac{\\partial}{\\partial \\mathbf{w}} (-y_i\\mathbf{w}^\\top\\mathbf{x}_i) = -y_i\\mathbf{x}_i$ porque la derivada de $\\mathbf{w}^\\top\\mathbf{x}_i$ con respecto a $\\mathbf{w}$ es simplemente $\\mathbf{x}_i$. Sustituyendo, obtenemos:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\mathbf{w}} \\log(1 + \\exp(-y_i\\mathbf{w}^\\top\\mathbf{x}_i)) = \\frac{\\exp(-y_i\\mathbf{w}^\\top\\mathbf{x}_i)}{1 + \\exp(-y_i\\mathbf{w}^\\top\\mathbf{x}_i)} (-y_i\\mathbf{x}_i) = (1 - \\sigma(\\mathbf{x}_i, y_i, \\mathbf{w})) (-y_i\\mathbf{x}_i) $$\n",
    "\n",
    "Agregando el término de la suma, tenemos que por regla de derivación de una suma simplemente se deriva cada término, teniendo unicamente que colocar la suma de todos los términos para $i = 1$ a $m$:\n",
    "\n",
    "$$ -\\sum_{i=1}^m (1 - \\sigma(\\mathbf{x}_i, y_i, \\mathbf{w})) y_i\\mathbf{x}_i $$\n",
    "\n",
    "Ahora derivamos el segundo término. El segundo término $\\frac{\\lambda}{2} \\mathbf{w}^\\top\\mathbf{w}$ es más sencillo, ya que es un término cuadrático en $\\mathbf{w}$. Su gradiente es:\n",
    "\n",
    "$$ \\nabla_w \\left(\\frac{\\lambda}{2} \\mathbf{w}^\\top\\mathbf{w}\\right) = \\lambda \\mathbf{w} $$\n",
    "\n",
    "Combinando los gradientes de ambos términos, obtenemos el gradiente completo de $L(\\mathbf{w})$ como:\n",
    "\n",
    "$$ \\nabla_w L(\\mathbf{w}) = - \\sum_{i=1}^m (1 - \\sigma(\\mathbf{x}_i, y_i, \\mathbf{w})) y_i\\mathbf{x}_i + \\lambda \\mathbf{w} $$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Programar las funciones \n",
    "\n",
    "$$\\sigma(\\mathbf{X}, \\mathbf{y}, \\mathbf{w}), \\quad \n",
    "L(\\mathbf{w})\\quad \\text{y} \\quad \\nabla_w L(\\mathbf{w}).$$\n",
    "   \n",
    "- Conviene programar la función sigmoide para que pueda recibir la matriz $\\mathbf{X}$ y el vector\n",
    "  $\\mathbf{y}$, en lugar de dar un vector $\\mathbf{x}_i$ y su etiqueta $y_i$, para que\n",
    "  evalue todos los datos y devuelva un vector con probabilidades\n",
    "  \n",
    "$$ \\left( \\begin{array}{c}\n",
    "\\sigma(\\mathbf{x}_1, y_1, \\mathbf{w})  \\\\\n",
    "\\sigma(\\mathbf{x}_2, y_2, \\mathbf{w}) \\\\\n",
    "\\vdots \\\\\n",
    "\\sigma(\\mathbf{x}_m, y_m, \\mathbf{w})\n",
    "\\end{array} \\right).\n",
    "$$ \n",
    "\n",
    "- Una vez que se tiene ese vector de probabilidades, se puede calcular el gradiente\n",
    "  de $ L()$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "# data modeling\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función sigma \n",
    "def sigmoid(X, y, w):\n",
    "    return 1/(1 + np.exp(-y * np.dot(w, X)))\n",
    "\n",
    "# Función L\n",
    "def L(w, x, y, l):\n",
    "    aux = 1 + np.exp(-y * np.dot(w, x)) + l/2.0 * np.dot(w, w)\n",
    "    return np.log(aux)\n",
    "\n",
    "# Gradiente de L\n",
    "def gradL(w, x, y, l, sigma):\n",
    "    aux = 1 - sigma(x, y, w) * y*x + l*w\n",
    "    return -aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Aplique el método de descenso máximo para minimizar la función $L(\\mathbf{w})$. \n",
    "   Use backtracking para calcular el tamaño de paso $\\alpha_k$, de modo \n",
    "   $\\mathbf{w}_{k+1} = \\mathbf{w}_{k} + \\alpha_k  \\mathbf{p}_{k}$, donde\n",
    "   \n",
    "$$\\mathbf{p}_{k} = - \\mathbf{g}_{k} = -\\nabla_w L(\\mathbf{w}_k)$$\n",
    "  \n",
    "  \n",
    "Una vez que se ha calculado el minimizador $\\mathbf{w}_*$ de $L(\\mathbf{w})$\n",
    "puede usar la función $predict(\\mathbf{X}, \\mathbf{w}_*)$, codificada en la siguiente celda,\n",
    "para predecir las etiquetas de los datos que están en la matriz $\\mathbf{X}$.\n",
    "Así, esta función devuelve un vector que tiene las etiquetas $-1$ o  $1$ que se asigna\n",
    "cada dato (fila) en la matriz $\\mathbf{X}$ de acuerdo a la probabilidad que tiene ese\n",
    "dato de pertenecer a una de las clases.\n",
    "\n",
    "**Nota:**   Hay que implementar la función `sigmoid()` como se indica en el Punto 2\n",
    "para poder ejecutar la función `predict()`.\n",
    "\n",
    "La función  `predict()` es la respuesta del clasificador en cada dato de la matriz $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para predecir la clase de cada dato (fila) en la matriz X\n",
    "# Devuelve un arreglo del tamaño de la cantidad de filas de X que tiene \n",
    "# las etiquetas -1 o 1  que se predicen para cada dato.\n",
    "# Para calcular las etiquetas, se calcula el vector que tiene las probabilidades\n",
    "# de que los datos pertenezcan a la clase 1. Si la probabilidad es mayor que 0.5,\n",
    "# se asigna la clase 1. En caso contrario se asigna la clase -1.\n",
    "#\n",
    "def predict(X, w):  \n",
    "    # Vector de prediciones. Se inicializa como si todas las etiquetas fueran 1\n",
    "    y_pred = np.ones(X.shape[0])\n",
    "    # Vector de probalidades de que los datos pertenezcan a la clase 1\n",
    "    vprob  = sigmoid(X, np.ones(X.shape[0]), w)\n",
    "    # Se obtienen los índices de los datos que tienen una probabilidad menor a 0.5\n",
    "    ii     = np.where(vprob<=0.5)[0]\n",
    "    # Se cambia la etiqueta por -1 para todos los datos con probabilidad menor a 0.5\n",
    "    y_pred[ii] = -1\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, dado un conjunto de datos, se toma una parte de ellos para construir\n",
    "el clasificador. Ese subconjunto se llama el **conjunto de entrenamiento**.\n",
    "El resto de los datos se usan para evaluar el desempeño del clasificador y \n",
    "se llama el **conjunto de prueba**.\n",
    "   \n",
    "Para evaluar el desempeño del clasificador hay varias métricas.\n",
    "El código de la siguiente celda muestra:\n",
    "- Cómo leer los datos de un archivo, \n",
    "- separarlos en el conjunto de entrenamiento y validación,\n",
    "- estandarizar los datos de cada conjunto,\n",
    "- agregar una columna formada por 1's a los datos. Si no se hace esto, \n",
    "  en lugar de usar el producto $\\mathbf{w}^\\top\\mathbf{x}_i$, se tendría que \n",
    "  usar $b + \\mathbf{w}^\\top\\mathbf{x}_i$ y calcular el bias $b$ por separado.\n",
    "  Al agregar esta columna de 1's a los datos, es como equivalente a que el bias \n",
    "  $b$ forme parte del vector $\\mathbf{w}$.\n",
    "- Se calcula la matriz de confusión que en su diagonal muestra la cantidad\n",
    "  de datos en los que la predicción de la clase que hace el clasificador es correcta,\n",
    "  mientras que los elementos fuera de la diagonal son la cantidad de datos\n",
    "  mal clasificados.\n",
    "- Se evalúa la exactitud (accuracy) del clasificador. Entre más cerca esté este\n",
    "  valor a 1, es mejor el desempeño del clasificador.\n",
    "\n",
    "\n",
    "El conjunto de datos corresponde a un estudio en el que se miden 13 características\n",
    "a una muestra de 303 individuos, descritas en \n",
    "\n",
    "[Heart disease](https://archive.ics.uci.edu/dataset/45/heart+disease)\n",
    "\n",
    "Cada registro  tiene una etiqueta que indica la presencia (etiqueta 1) de una\n",
    "enfermedad del corazón, o que no la tiene (etiqueta 0). Esta última etiqueta\n",
    "la cambiamos por \"-1\" para que coincida con la notación del artículo.\n",
    "\n",
    "El objetivo es tomar una parte de los datos para crear el clasificador y medir\n",
    "el desempeño del clasificador con el resto los datos, haciendo que el clasificador\n",
    "prediga a que clase pertenece cada dato del conjunto de prueba y comparando \n",
    "las predicciones con la verdadera etiqueta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de la tabla: (303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lectura de los datos\n",
    "data = pd.read_csv('heart.csv')\n",
    "print('Dimensiones de la tabla:', data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    138\n",
       "1    165\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Esto muestra cuántos datos se tienen en la clase '0' y en la clase '1'\n",
    "data.groupby(['target']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = \n",
      "[ 0.11505553 -0.07527989 -0.86343872  0.79668329 -0.19306509 -0.24740666\n",
      " -0.13375708  0.09199133  0.5056945  -0.47893713 -0.64523259  0.13478362\n",
      " -0.88420603 -0.46002338]\n"
     ]
    }
   ],
   "source": [
    "# Cambiamos la etiqueta 0 por -1\n",
    "data.loc[data['target']==0, 'target'] = -1\n",
    "# Vector de etiquetas\n",
    "y  = data[\"target\"]\n",
    "\n",
    "# Matriz de datos\n",
    "X  = data.drop('target',axis=1)\n",
    "\n",
    "# Se usa el 20% de los datos para crear el conjunto de prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)\n",
    "\n",
    "# Se estandariza cada columna de la matriz de datos para evitar que por tener diferentes \n",
    "# rangos de valores cada columna (variable), afecte al algoritmo de optimización\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Número de muestras del conjunto de entrenamiento\n",
    "ntrain  = X_train.shape[0]\n",
    "# Se agrega una columna de 1's para que el bias b forme pare del vector w\n",
    "X_train = np.hstack((np.ones((ntrain,1)), X_train))\n",
    "\n",
    "# Número de muestras del conjunto de prueba\n",
    "ntest   = X_test.shape[0]\n",
    "# Se agrega una columna de 1's para que el bias b forme pare del vector w\n",
    "X_test  = np.hstack((np.ones((ntest,1)), X_test))\n",
    "\n",
    "# Se convierte los dataframes a una matriz de numpy\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "# Se entrena el clasificador de regresión logística\n",
    "lr    = LogisticRegression(fit_intercept=False)\n",
    "model = lr.fit(X_train, y_train)\n",
    "\n",
    "# Imprimimos las componentes de w\n",
    "w = np.squeeze(model.coef_)\n",
    "print('w = ')\n",
    "print(w)\n",
    "\n",
    "# Se calcula las predicciones para el conjunto de prueba \n",
    "y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.8360655737704918 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x12f0f11d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Se mide el desempeño calculando la matriz de confusión y la exactitud\n",
    "conf_matrix = confusion_matrix(y_test, y_predict)\n",
    "acc_score = accuracy_score(y_test, y_predict)\n",
    "print(\"\\nAccuracy:\", acc_score,'\\n')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,\n",
    "                              display_labels=model.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Pruebe el algoritmo de optimización usando $\\mathbf{w}_0=(1,1,...,1)$,  el \n",
    "   número de iteraciones máximas $N=500$, la tolerancia para terminar el algoritmo \n",
    "   $\\tau = \\sqrt{ntrain}\\epsilon_m^{1/3}$ y para el algoritmo de backtracking\n",
    "   $\\rho = 0.5, c_1 = 0.001$ $N_b=500$.\n",
    "   \n",
    "   Cree un clasificador usando $\\lambda=0.001$ y otro clasificador usando $\\lambda=1.0$. \n",
    "   \n",
    "   En cada caso use la función $predict(X_{test}, w_*)$\n",
    "   para obtener el vector de predicciones de la clase para el conjunto de prueba y\n",
    "   use el código de la celda anterior para obtener la matriz de confusión y la \n",
    "   exactitud del clasificador, para ver cual de los dos tiene mejor desempeño.\n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Opti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
